{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./images/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 每个单元的rnn结构\n",
    "def rnn_cell_forward(xt,a_prev,parameters):\n",
    "    '''\n",
    "    实现rnn单元的前向传播\n",
    "    参数：\n",
    "        xt -- 时间t输入的数据，维度为(n_x,m)\n",
    "        a_prev -- 时间步t-1的隐藏状态，维度为(n_a,m)\n",
    "        parameters -- 字典，包含了以下内容：\n",
    "                             Wax -- 矩阵，输入乘以权重，维度为(n_a,n_x)\n",
    "                             Waa -- 矩阵，隐藏状态乘以权重，维度为(n_a,n_a)\n",
    "                             Wya -- 矩阵，隐藏状态和输出相关的权重矩阵，维度为(n_y,n_a)\n",
    "                             ba   --  偏置，维度为(n_a,1)\n",
    "                             by   --  偏置，维度为(n_y,1)\n",
    "    返回：\n",
    "        a_next -- 下一个隐藏状态，维度为(n_a,m)\n",
    "        yt_pred -- 在时间t的预测，维度为(n_y,m)\n",
    "        cache -- 反向传播需要的元组，包含了(a_next,a_prev,xt,parameters)\n",
    "    '''\n",
    "    Wax = parameters['Wax']\n",
    "    Waa = parameters['Waa']\n",
    "    Wya = parameters['Wya']\n",
    "    ba = parameters['ba']\n",
    "    by = parameters['by']\n",
    "    # 计算a<t>\n",
    "    a_next = np.tanh(np.dot(Wax,xt) + np.dot(Waa,a_prev) + ba)\n",
    "    # 计算当前单元的输出\n",
    "    yt_pred = rnn_utils.softmax(np.dot(Wya,a_next) + by)\n",
    "    # \n",
    "    cache = (a_next,a_prev,xt,parameters)\n",
    "    \n",
    "    return a_next,yt_pred,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
      " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
      "a_next.shape =  (5, 10)\n",
      "yt_pred[1] = [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
      " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
      "yt_pred.shape =  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", a_next.shape)\n",
    "print(\"yt_pred[1] =\", yt_pred[1])\n",
    "print(\"yt_pred.shape = \", yt_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./images/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn网络\n",
    "def rnn_forward(x,a0,parameters):\n",
    "    '''\n",
    "    根据上图实现神经网络的前向传播\n",
    "        参数：\n",
    "        x -- 输入的全部数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为 (n_a, m)\n",
    "        parameters -- 字典，包含了以下内容:\n",
    "                        Wax -- 矩阵，输入乘以权重，维度为（n_a, n_x）\n",
    "                        Waa -- 矩阵，隐藏状态乘以权重，维度为（n_a, n_a）\n",
    "                        Wya -- 矩阵，隐藏状态与输出相关的权重矩阵，维度为（n_y, n_a）\n",
    "                        ba  -- 偏置，维度为（n_a, 1）\n",
    "                        by  -- 偏置，隐藏状态与输出相关的偏置，维度为（n_y, 1）\n",
    "    \n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y_pred -- 所有时间步的预测，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    '''\n",
    "    caches = []\n",
    "    # 获取x与Wya的维度信息\n",
    "    n_x,m,T_x = x.shape\n",
    "    n_y,n_a = parameters['Wya'].shape\n",
    "    \n",
    "    # 使用0来初始化‘a'和’y’\n",
    "    a = np.zeros([n_a,m,T_x])\n",
    "    y_pred = np.zeros([n_y,m,T_x])\n",
    "    \n",
    "    # 初始化'next\n",
    "    a_next =a0\n",
    "    \n",
    "    for t in range(T_x):\n",
    "        ## 1.使用rnn_cell_forward函数来更新“next”隐藏状态与cache。\n",
    "        a_next,yt_pred,cache = rnn_cell_forward(x[:,:,t],a_next,parameters)\n",
    "        ## 2.使用a来保存‘next’隐藏状态第（t）个位置\n",
    "        a[:,:,t] = a_next\n",
    "        ## 3.使用y来保存预测值\n",
    "        y_pred[:,:,t] = yt_pred\n",
    "        ## 4. 保存cache\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # 保存反向传播所需要的参数\n",
    "    caches = (caches,x)\n",
    "    \n",
    "    return a,y_pred,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
      "a.shape =  (5, 10, 4)\n",
      "y_pred[1][3] = [0.79560373 0.86224861 0.11118257 0.81515947]\n",
      "y_pred.shape =  (2, 10, 4)\n",
      "caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wax = np.random.randn(5,3)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Waa\": Waa, \"Wax\": Wax, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a, y_pred, caches = rnn_forward(x, a0, parameters)\n",
    "print(\"a[4][1] = \", a[4][1])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y_pred[1][3] =\", y_pred[1][3])\n",
    "print(\"y_pred.shape = \", y_pred.shape)\n",
    "print(\"caches[1][1][3] =\", caches[1][1][3])\n",
    "print(\"len(caches) = \", len(caches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./images/3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM (Long-Short Term Memory)\n",
    "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
    "    '''\n",
    "   根据图4实现一个LSTM单元的前向传播。\n",
    "\n",
    "   参数：\n",
    "       xt -- 在时间步“t”输入的数据，维度为(n_x, m)\n",
    "       a_prev -- 上一个时间步“t-1”的隐藏状态，维度为(n_a, m)\n",
    "       c_prev -- 上一个时间步“t-1”的记忆状态，维度为(n_a, m)\n",
    "       parameters -- 字典类型的变量，包含了：\n",
    "                       Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                       bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                       Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                       bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                       Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                       bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                       Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                       bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                       Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                       by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "   返回：\n",
    "       a_next -- 下一个隐藏状态，维度为(n_a, m)\n",
    "       c_next -- 下一个记忆状态，维度为(n_a, m)\n",
    "       yt_pred -- 在时间步“t”的预测，维度为(n_y, m)\n",
    "       cache -- 包含了反向传播所需要的参数，包含了(a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "\n",
    "   注意：\n",
    "       ft/it/ot表示遗忘/更新/输出门，cct表示候选值(c tilda)，c表示记忆值。\n",
    "   '''\n",
    "\n",
    "    # 从“parameters”中获取相关值\n",
    "\n",
    "\n",
    "    Wf = parameters[\"Wf\"]\n",
    "    bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]\n",
    "    bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]\n",
    "    bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]\n",
    "    bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 获取xt与Wy的维度信息\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "    # 连接a_prev与xt\n",
    "    contact = np.zeros([n_a + n_x, m])\n",
    "    contact[:n_a, :] = a_prev\n",
    "    contact[n_a:, :] = xt\n",
    "    \n",
    "    # 根据公式进行计算ft,it,cct,c_next,ot,,a_next\n",
    "    ## 遗忘门，公式1\n",
    "    ft = rnn_utils.sigmoid(np.dot(Wf, contact) + bf)\n",
    "    ## 更新门，公式2\n",
    "    it = rnn_utils.sigmoid(np.dot(Wi, contact) + bi)\n",
    "    ## 更新单元，cct,公式3\n",
    "    cct = np.tanh(np.dot(Wc, contact) + bc)\n",
    "    ## 更新单元，公式4\n",
    "    c_next = ft * c_prev + it * cct\n",
    "    ## 输出门，公式5\n",
    "    ot = rnn_utils.sigmoid(np.dot(Wo, contact) + bo)\n",
    "    ## 输出门，公式6\n",
    "    a_next = ot * np.tanh(c_next)\n",
    "    # 计算该单元的预测值\n",
    "    yt_pred = rnn_utils.softmax(np.dot(Wy, a_next) + by)\n",
    "    # 保存包含了反向传播需要的参数\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "    \n",
    "    return a_next, c_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
      "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
      "a_next.shape =  (5, 10)\n",
      "c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
      "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
      "c_next.shape =  (5, 10)\n",
      "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
      " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
      "yt.shape =  (2, 10)\n",
      "cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
      "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
      "len(cache) =  10\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "print(\"a_next[4] = \", a_next[4])\n",
    "print(\"a_next.shape = \", c_next.shape)\n",
    "print(\"c_next[2] = \", c_next[2])\n",
    "print(\"c_next.shape = \", c_next.shape)\n",
    "print(\"yt[1] =\", yt[1])\n",
    "print(\"yt.shape = \", yt.shape)\n",
    "print(\"cache[1][3] =\", cache[1][3])\n",
    "print(\"len(cache) = \", len(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./images/4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_forward(x, a0, parameters):\n",
    "    '''\n",
    "        根据上图来实现LSTM单元组成的的循环神经网络\n",
    "    \n",
    "    参数：\n",
    "        x -- 所有时间步的输入数据，维度为(n_x, m, T_x)\n",
    "        a0 -- 初始化隐藏状态，维度为(n_a, m)\n",
    "        parameters -- python字典，包含了以下参数：\n",
    "                        Wf -- 遗忘门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bf -- 遗忘门的偏置，维度为(n_a, 1)\n",
    "                        Wi -- 更新门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bi -- 更新门的偏置，维度为(n_a, 1)\n",
    "                        Wc -- 第一个“tanh”的权值，维度为(n_a, n_a + n_x)\n",
    "                        bc -- 第一个“tanh”的偏置，维度为(n_a, n_a + n_x)\n",
    "                        Wo -- 输出门的权值，维度为(n_a, n_a + n_x)\n",
    "                        bo -- 输出门的偏置，维度为(n_a, 1)\n",
    "                        Wy -- 隐藏状态与输出相关的权值，维度为(n_y, n_a)\n",
    "                        by -- 隐藏状态与输出相关的偏置，维度为(n_y, 1)\n",
    "        \n",
    "    返回：\n",
    "        a -- 所有时间步的隐藏状态，维度为(n_a, m, T_x)\n",
    "        y -- 所有时间步的预测值，维度为(n_y, m, T_x)\n",
    "        caches -- 为反向传播的保存的元组，维度为（【列表类型】cache, x)）\n",
    "    '''\n",
    "    caches = []\n",
    "    # 获取xt与Wy的维度\n",
    "    n_x,m,T_x = x.shape\n",
    "    n_y,n_a = parameters['Wy'].shape\n",
    "    # 用0来初始化a,c,y\n",
    "    a = np.zeros([n_a,m,T_x])\n",
    "    c = np.zeros([n_a,m,T_x])\n",
    "    y = np.zeros([n_y,m,T_x])\n",
    "    #初始化a_next,c_next\n",
    "    a_next = a0\n",
    "    c_next =np.zeros([n_a,m])\n",
    "    \n",
    "    # 遍历所有的时间步\n",
    "    for t in range(T_x):\n",
    "        # 更新下一个隐藏状态，下一个记忆状态，计算预测值，获取cache\n",
    "        a_next,c_next,yt_pred,cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)\n",
    "        a[:,:,t] = a_next\n",
    "        c[:,:,t] = c_next\n",
    "        y[:,:,t] = yt_pred\n",
    "        caches.append(cache)\n",
    "    caches = (caches,x)\n",
    "    \n",
    "    return a,y,c,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a[4][3][6] =  0.17211776753291672\n",
      "a.shape =  (5, 10, 7)\n",
      "y[1][4][3] = 0.9508734618501101\n",
      "y.shape =  (2, 10, 7)\n",
      "caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
      "  0.41005165]\n",
      "c[1][2][1] -0.8555449167181981\n",
      "len(caches) =  2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "print(\"a[4][3][6] = \", a[4][3][6])\n",
    "print(\"a.shape = \", a.shape)\n",
    "print(\"y[1][4][3] =\", y[1][4][3])\n",
    "print(\"y.shape = \", y.shape)\n",
    "print(\"caches[1][1[1]] =\", caches[1][1][1])\n",
    "print(\"c[1][2][1]\", c[1][2][1])\n",
    "print(\"len(caches) = \", len(caches))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./images/5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell_backward(da_next, cache):\n",
    "    \"\"\"\n",
    "    实现基本的RNN单元的单步反向传播\n",
    "    \n",
    "    参数：\n",
    "        da_next -- 关于下一个隐藏状态的损失的梯度。\n",
    "        cache -- 字典类型，rnn_step_forward()的输出\n",
    "        \n",
    "    返回：\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dx -- 输入数据的梯度，维度为(n_x, m)\n",
    "                        da_prev -- 上一隐藏层的隐藏状态，维度为(n_a, m)\n",
    "                        dWax -- 输入到隐藏状态的权重的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏状态到隐藏状态的权重的梯度，维度为(n_a, n_a)\n",
    "                        dba -- 偏置向量的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # 获取cache 的值\n",
    "    a_next, a_prev, xt, parameters = cache\n",
    "    \n",
    "    # 从 parameters 中获取参数\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # 计算tanh相对于a_next的梯度.\n",
    "    dtanh = (1 - np.square(a_next)) * da_next\n",
    "    \n",
    "    # 计算关于Wax损失的梯度\n",
    "    dxt = np.dot(Wax.T,dtanh)\n",
    "    dWax = np.dot(dtanh, xt.T)\n",
    "    \n",
    "    # 计算关于Waa损失的梯度\n",
    "    da_prev = np.dot(Waa.T,dtanh)\n",
    "    dWaa = np.dot(dtanh, a_prev.T)\n",
    "    \n",
    "    # 计算关于b损失的梯度\n",
    "    dba = np.sum(dtanh, keepdims=True, axis=-1)\n",
    "    \n",
    "    # 保存这些梯度到字典内\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dxt\"][1][2] = -0.4605641030588796\n",
      "gradients[\"dxt\"].shape = (3, 10)\n",
      "gradients[\"da_prev\"][2][3] = 0.08429686538067718\n",
      "gradients[\"da_prev\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 0.3930818739219304\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = -0.2848395578696067\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [0.80517166]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "a_next, yt, cache = rnn_cell_forward(xt, a_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "gradients = rnn_cell_backward(da_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(da, caches):\n",
    "    \"\"\"\n",
    "    在整个输入数据序列上实现RNN的反向传播\n",
    "    \n",
    "    参数：\n",
    "        da -- 所有隐藏状态的梯度，维度为(n_a, m, T_x)\n",
    "        caches -- 包含向前传播的信息的元组\n",
    "    \n",
    "    返回：    \n",
    "        gradients -- 包含了梯度的字典：\n",
    "                        dx -- 关于输入数据的梯度，维度为(n_x, m, T_x)\n",
    "                        da0 -- 关于初始化隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dWax -- 关于输入权重的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 关于隐藏状态的权值的梯度，维度为(n_a, n_a)\n",
    "                        dba -- 关于偏置的梯度，维度为(n_a, 1)\n",
    "    \"\"\"\n",
    "    # 从caches中获取第一个cache（t=1）的值\n",
    "    caches, x = caches\n",
    "    a1, a0, x1, parameters = caches[0]\n",
    "    \n",
    "    # 获取da与x1的维度信息\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # 初始化梯度\n",
    "    dx = np.zeros([n_x, m, T_x])\n",
    "    dWax = np.zeros([n_a, n_x])\n",
    "    dWaa = np.zeros([n_a, n_a])\n",
    "    dba = np.zeros([n_a, 1])\n",
    "    da0 = np.zeros([n_a, m])\n",
    "    da_prevt = np.zeros([n_a, m])\n",
    "    \n",
    "    # 处理所有时间步\n",
    "    for t in reversed(range(T_x)):\n",
    "        # 计算时间步“t”时的梯度\n",
    "        gradients = rnn_cell_backward(da[:, :, t] + da_prevt, caches[t])\n",
    "        \n",
    "        #从梯度中获取导数\n",
    "        dxt, da_prevt, dWaxt, dWaat, dbat = gradients[\"dxt\"], gradients[\"da_prev\"], gradients[\"dWax\"], gradients[\"dWaa\"], gradients[\"dba\"]\n",
    "        \n",
    "        # 通过在时间步t添加它们的导数来增加关于全局导数的参数\n",
    "        dx[:, :, t] = dxt\n",
    "        dWax += dWaxt\n",
    "        dWaa += dWaat\n",
    "        dba += dbat\n",
    "        \n",
    "    #将 da0设置为a的梯度，该梯度已通过所有时间步骤进行反向传播\n",
    "    da0 = da_prevt\n",
    "    \n",
    "    #保存这些梯度到字典内\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWax\": dWax, \"dWaa\": dWaa,\"dba\": dba}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-2.07101689 -0.59255627  0.02466855  0.01483317]\n",
      "gradients[\"dx\"].shape = (3, 10, 4)\n",
      "gradients[\"da0\"][2][3] = -0.31494237512664996\n",
      "gradients[\"da0\"].shape = (5, 10)\n",
      "gradients[\"dWax\"][3][1] = 11.264104496527777\n",
      "gradients[\"dWax\"].shape = (5, 3)\n",
      "gradients[\"dWaa\"][1][2] = 2.3033331265798935\n",
      "gradients[\"dWaa\"].shape = (5, 5)\n",
      "gradients[\"dba\"][4] = [-0.74747722]\n",
      "gradients[\"dba\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,4)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wax = np.random.randn(5,3)\n",
    "Waa = np.random.randn(5,5)\n",
    "Wya = np.random.randn(2,5)\n",
    "ba = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "a, y, caches = rnn_forward(x, a0, parameters)\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = rnn_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWax\\\"].shape =\", gradients[\"dWax\"].shape)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients[\"dWaa\"].shape)\n",
    "print(\"gradients[\\\"dba\\\"][4] =\", gradients[\"dba\"][4])\n",
    "print(\"gradients[\\\"dba\\\"].shape =\", gradients[\"dba\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](./images/6.png)\n",
    "![avatar](./images/7.png)\n",
    "![avatar](./images/8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lstm的反向传播\n",
    "def lstm_cell_backward(da_next,dc_next,cache):\n",
    "    '''\n",
    "    实现LSTM的单步反向传播\n",
    "    参数：\n",
    "        da_next -- 下一个隐藏状态的梯度，维度为(n_a,m)\n",
    "        dc_next -- 下一个单元状态的梯度，维度为(n_a,m)\n",
    "        cache -- 来自前向传播的一些参数\n",
    "    返回：\n",
    "        gradients -- 包含了梯度信息的字典：\n",
    "                    dxt -- 输入数据的梯度，维度为(n_x, m)\n",
    "                    da_prev -- 先前的隐藏状态的梯度，维度为(n_a, m)\n",
    "                    dc_prev -- 前的记忆状态的梯度，维度为(n_a, m, T_x)\n",
    "                    dWf -- 遗忘门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                    dbf -- 遗忘门的偏置的梯度，维度为(n_a, 1)\n",
    "                    dWi -- 更新门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                    dbi -- 更新门的偏置的梯度，维度为(n_a, 1)\n",
    "                    dWc -- 第一个“tanh”的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                    dbc -- 第一个“tanh”的偏置的梯度，维度为(n_a, n_a + n_x)\n",
    "                    dWo -- 输出门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                    dbo -- 输出门的偏置的梯度，维度为(n_a, 1)\n",
    "    '''\n",
    "    # 从cache中获取信息\n",
    "    (a_next,c_next,a_prev,c_prev,ft,it,cct,ot,xt,parameters) = cache\n",
    "    # 获取xt和a_next的维度信息\n",
    "    n_x,m = xt.shape\n",
    "    n_a,m = a_next.shape\n",
    "    \n",
    "     # 根据公式来计算门的导数\n",
    "    dot = da_next * np.tanh(c_next) * ot * (1 - ot)\n",
    "    dcct = (dc_next * it + ot * (1 - np.square(np.tanh(c_next))) * it * da_next) * (1 - np.square(cct))\n",
    "    dit = (dc_next * cct + ot * (1 - np.square(np.tanh(c_next))) * cct * da_next) * it * (1 - it)\n",
    "    dft = (dc_next * c_prev + ot * (1 - np.square(np.tanh(c_next))) * c_prev * da_next) * ft * (1 - ft)\n",
    "    \n",
    "    # 根据公式计算参数的导数\n",
    "    concat = np.concatenate((a_prev, xt), axis=0).T\n",
    "    dWf = np.dot(dft, concat)\n",
    "    dWi = np.dot(dit, concat)\n",
    "    dWc = np.dot(dcct, concat)\n",
    "    dWo = np.dot(dot, concat)\n",
    "    dbf = np.sum(dft,axis=1,keepdims=True)\n",
    "    dbi = np.sum(dit,axis=1,keepdims=True)\n",
    "    dbc = np.sum(dcct,axis=1,keepdims=True)\n",
    "    dbo = np.sum(dot,axis=1,keepdims=True)\n",
    "    # 使用公式计算洗起来了隐藏状态、先前记忆状态、输入的导数。\n",
    "    da_prev = np.dot(parameters[\"Wf\"][:, :n_a].T, dft) + np.dot(parameters[\"Wc\"][:, :n_a].T, dcct) +  np.dot(parameters[\"Wi\"][:, :n_a].T, dit) + np.dot(parameters[\"Wo\"][:, :n_a].T, dot)\n",
    "        \n",
    "    dc_prev = dc_next * ft + ot * (1 - np.square(np.tanh(c_next))) * ft * da_next\n",
    "    \n",
    "    dxt = np.dot(parameters[\"Wf\"][:, n_a:].T, dft) + np.dot(parameters[\"Wc\"][:, n_a:].T, dcct) +  np.dot(parameters[\"Wi\"][:, n_a:].T, dit) + np.dot(parameters[\"Wo\"][:, n_a:].T, dot)\n",
    "    \n",
    "    # 保存梯度信息到字典\n",
    "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dc_prev\": dc_prev, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dxt\"][1][2] = 3.230559115109188\n",
      "gradients[\"dxt\"].shape = (3, 10)\n",
      "gradients[\"da_prev\"][2][3] = -0.06396214197109236\n",
      "gradients[\"da_prev\"].shape = (5, 10)\n",
      "gradients[\"dc_prev\"][2][3] = 0.7975220387970015\n",
      "gradients[\"dc_prev\"].shape = (5, 10)\n",
      "gradients[\"dWf\"][3][1] = -0.1479548381644968\n",
      "gradients[\"dWf\"].shape = (5, 8)\n",
      "gradients[\"dWi\"][1][2] = 1.0574980552259903\n",
      "gradients[\"dWi\"].shape = (5, 8)\n",
      "gradients[\"dWc\"][3][1] = 2.3045621636876668\n",
      "gradients[\"dWc\"].shape = (5, 8)\n",
      "gradients[\"dWo\"][1][2] = 0.3313115952892109\n",
      "gradients[\"dWo\"].shape = (5, 8)\n",
      "gradients[\"dbf\"][4] = [0.18864637]\n",
      "gradients[\"dbf\"].shape = (5, 1)\n",
      "gradients[\"dbi\"][4] = [-0.40142491]\n",
      "gradients[\"dbi\"].shape = (5, 1)\n",
      "gradients[\"dbc\"][4] = [0.25587763]\n",
      "gradients[\"dbc\"].shape = (5, 1)\n",
      "gradients[\"dbo\"][4] = [0.13893342]\n",
      "gradients[\"dbo\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "xt = np.random.randn(3,10)\n",
    "a_prev = np.random.randn(5,10)\n",
    "c_prev = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "Wy = np.random.randn(2,5)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)\n",
    "\n",
    "da_next = np.random.randn(5,10)\n",
    "dc_next = np.random.randn(5,10)\n",
    "gradients = lstm_cell_backward(da_next, dc_next, cache)\n",
    "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients[\"dxt\"][1][2])\n",
    "print(\"gradients[\\\"dxt\\\"].shape =\", gradients[\"dxt\"].shape)\n",
    "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients[\"da_prev\"][2][3])\n",
    "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients[\"da_prev\"].shape)\n",
    "print(\"gradients[\\\"dc_prev\\\"][2][3] =\", gradients[\"dc_prev\"][2][3])\n",
    "print(\"gradients[\\\"dc_prev\\\"].shape =\", gradients[\"dc_prev\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_backward(da, caches):\n",
    "    \n",
    "    \"\"\"\n",
    "    实现LSTM网络的反向传播\n",
    "    \n",
    "    参数：\n",
    "        da -- 关于隐藏状态的梯度，维度为(n_a, m, T_x)\n",
    "        cachses -- 前向传播保存的信息\n",
    "    \n",
    "    返回：\n",
    "        gradients -- 包含了梯度信息的字典：\n",
    "                        dx -- 输入数据的梯度，维度为(n_x, m，T_x)\n",
    "                        da0 -- 先前的隐藏状态的梯度，维度为(n_a, m)\n",
    "                        dWf -- 遗忘门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbf -- 遗忘门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWi -- 更新门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbi -- 更新门的偏置的梯度，维度为(n_a, 1)\n",
    "                        dWc -- 第一个“tanh”的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbc -- 第一个“tanh”的偏置的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dWo -- 输出门的权值的梯度，维度为(n_a, n_a + n_x)\n",
    "                        dbo -- 输出门的偏置的梯度，维度为(n_a, 1)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # 从caches中获取第一个cache（t=1）的值\n",
    "    caches, x = caches\n",
    "    (a1, c1, a0, c0, f1, i1, cc1, o1, x1, parameters) = caches[0]\n",
    "    \n",
    "    # 获取da与x1的维度信息\n",
    "    n_a, m, T_x = da.shape\n",
    "    n_x, m = x1.shape\n",
    "    \n",
    "    # 初始化梯度\n",
    "    dx = np.zeros([n_x, m, T_x])\n",
    "    da0 = np.zeros([n_a, m])\n",
    "    da_prevt = np.zeros([n_a, m])\n",
    "    dc_prevt = np.zeros([n_a, m])\n",
    "    dWf = np.zeros([n_a, n_a + n_x])\n",
    "    dWi = np.zeros([n_a, n_a + n_x])\n",
    "    dWc = np.zeros([n_a, n_a + n_x])\n",
    "    dWo = np.zeros([n_a, n_a + n_x])\n",
    "    dbf = np.zeros([n_a, 1])\n",
    "    dbi = np.zeros([n_a, 1])\n",
    "    dbc = np.zeros([n_a, 1])\n",
    "    dbo = np.zeros([n_a, 1])\n",
    "    \n",
    "    # 处理所有时间步\n",
    "    for t in reversed(range(T_x)):\n",
    "        # 使用lstm_cell_backward函数计算所有梯度\n",
    "        gradients = lstm_cell_backward(da[:,:,t],dc_prevt,caches[t])\n",
    "        # 保存相关参数\n",
    "        dx[:,:,t] = gradients['dxt']\n",
    "        dWf = dWf+gradients['dWf']\n",
    "        dWi = dWi+gradients['dWi']\n",
    "        dWc = dWc+gradients['dWc']\n",
    "        dWo = dWo+gradients['dWo']\n",
    "        dbf = dbf+gradients['dbf']\n",
    "        dbi = dbi+gradients['dbi']\n",
    "        dbc = dbc+gradients['dbc']\n",
    "        dbo = dbo+gradients['dbo']\n",
    "    # 将第一个激活的梯度设置为反向传播的梯度da_prev。\n",
    "    da0 = gradients['da_prev']\n",
    "\n",
    "    # 保存所有梯度到字典变量内\n",
    "    gradients = {\"dx\": dx, \"da0\": da0, \"dWf\": dWf,\"dbf\": dbf, \"dWi\": dWi,\"dbi\": dbi,\n",
    "                \"dWc\": dWc,\"dbc\": dbc, \"dWo\": dWo,\"dbo\": dbo}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dx\"][1][2] = [-0.00173313  0.08287442 -0.30545663 -0.43281115]\n",
      "gradients[\"dx\"].shape = (3, 10, 4)\n",
      "gradients[\"da0\"][2][3] = -0.09591150195400468\n",
      "gradients[\"da0\"].shape = (5, 10)\n",
      "gradients[\"dWf\"][3][1] = -0.06981985612744009\n",
      "gradients[\"dWf\"].shape = (5, 8)\n",
      "gradients[\"dWi\"][1][2] = 0.10237182024854771\n",
      "gradients[\"dWi\"].shape = (5, 8)\n",
      "gradients[\"dWc\"][3][1] = -0.062498379492745226\n",
      "gradients[\"dWc\"].shape = (5, 8)\n",
      "gradients[\"dWo\"][1][2] = 0.04843891314443012\n",
      "gradients[\"dWo\"].shape = (5, 8)\n",
      "gradients[\"dbf\"][4] = [-0.0565788]\n",
      "gradients[\"dbf\"].shape = (5, 1)\n",
      "gradients[\"dbi\"][4] = [-0.15399065]\n",
      "gradients[\"dbi\"].shape = (5, 1)\n",
      "gradients[\"dbc\"][4] = [-0.29691142]\n",
      "gradients[\"dbc\"].shape = (5, 1)\n",
      "gradients[\"dbo\"][4] = [-0.29798344]\n",
      "gradients[\"dbo\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(3,10,7)\n",
    "a0 = np.random.randn(5,10)\n",
    "Wf = np.random.randn(5, 5+3)\n",
    "bf = np.random.randn(5,1)\n",
    "Wi = np.random.randn(5, 5+3)\n",
    "bi = np.random.randn(5,1)\n",
    "Wo = np.random.randn(5, 5+3)\n",
    "bo = np.random.randn(5,1)\n",
    "Wc = np.random.randn(5, 5+3)\n",
    "bc = np.random.randn(5,1)\n",
    "\n",
    "parameters = {\"Wf\": Wf, \"Wi\": Wi, \"Wo\": Wo, \"Wc\": Wc, \"Wy\": Wy, \"bf\": bf, \"bi\": bi, \"bo\": bo, \"bc\": bc, \"by\": by}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, parameters)\n",
    "\n",
    "da = np.random.randn(5, 10, 4)\n",
    "gradients = lstm_backward(da, caches)\n",
    "\n",
    "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients[\"dx\"][1][2])\n",
    "print(\"gradients[\\\"dx\\\"].shape =\", gradients[\"dx\"].shape)\n",
    "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients[\"da0\"][2][3])\n",
    "print(\"gradients[\\\"da0\\\"].shape =\", gradients[\"da0\"].shape)\n",
    "print(\"gradients[\\\"dWf\\\"][3][1] =\", gradients[\"dWf\"][3][1])\n",
    "print(\"gradients[\\\"dWf\\\"].shape =\", gradients[\"dWf\"].shape)\n",
    "print(\"gradients[\\\"dWi\\\"][1][2] =\", gradients[\"dWi\"][1][2])\n",
    "print(\"gradients[\\\"dWi\\\"].shape =\", gradients[\"dWi\"].shape)\n",
    "print(\"gradients[\\\"dWc\\\"][3][1] =\", gradients[\"dWc\"][3][1])\n",
    "print(\"gradients[\\\"dWc\\\"].shape =\", gradients[\"dWc\"].shape)\n",
    "print(\"gradients[\\\"dWo\\\"][1][2] =\", gradients[\"dWo\"][1][2])\n",
    "print(\"gradients[\\\"dWo\\\"].shape =\", gradients[\"dWo\"].shape)\n",
    "print(\"gradients[\\\"dbf\\\"][4] =\", gradients[\"dbf\"][4])\n",
    "print(\"gradients[\\\"dbf\\\"].shape =\", gradients[\"dbf\"].shape)\n",
    "print(\"gradients[\\\"dbi\\\"][4] =\", gradients[\"dbi\"][4])\n",
    "print(\"gradients[\\\"dbi\\\"].shape =\", gradients[\"dbi\"].shape)\n",
    "print(\"gradients[\\\"dbc\\\"][4] =\", gradients[\"dbc\"][4])\n",
    "print(\"gradients[\\\"dbc\\\"].shape =\", gradients[\"dbc\"].shape)\n",
    "print(\"gradients[\\\"dbo\\\"][4] =\", gradients[\"dbo\"][4])\n",
    "print(\"gradients[\\\"dbo\\\"].shape =\", gradients[\"dbo\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dinosaur Island （恐龙岛问题）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import cllm_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'j', 'e', 'y', 'f', 't', 'u', 'g', 'i', 'x', 'p', '\\n', 'r', 'a', 'w', 'o', 'd', 'v', 'n', 'h', 'm', 's', 'z', 'q', 'b', 'l', 'k']\n",
      "共计有19909个名字，27个唯一字符\n"
     ]
    }
   ],
   "source": [
    "# 获取名称\n",
    "data = open('dinos.txt','r').read()\n",
    "data = data.lower()\n",
    "# 转化成无序且不重复的元素列表\n",
    "chars = list(set(data))\n",
    "data_size,vocab_size = len(data),len(chars)\n",
    "print(chars)\n",
    "print('共计有{0}个名字，{1}个唯一字符'.format(data_size,vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# 创建字典索引\n",
    "char_to_ix = {ch:i for i, ch in enumerate(sorted(chars))}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(sorted(chars))}\n",
    "\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients,maxValue):\n",
    "    '''\n",
    "    使用maxValue来修剪梯度\n",
    "    参数：\n",
    "        gradients -- 字典类型，包含了以下参数:'dWaa','dWax','dWya','db','dby'\n",
    "        maxValue -- 阈值,将梯度限制在[-maxValue,maxValue]\n",
    "    返回：\n",
    "        gradients -- 修剪后的梯度\n",
    "    '''\n",
    "    # 获取参数：\n",
    "    dWaa,dWax,dWya,db,dby = gradients['dWaa'],gradients['dWax'],gradients['dWya'],gradients['db'],gradients['dby']\n",
    "    # 梯度修剪\n",
    "    for gradient in [dWaa,dWax,dWya,db,dby]:\n",
    "        np.clip(gradient,-maxValue,maxValue,out=gradient)\n",
    "    gradients = {'dWaa':dWaa,'dWax':dWax,'dWya':dWya,'db':db,'dby':dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "p = np.array([0.1,0.0,0.7,0.2])\n",
    "index = np.random.choice([0,1,2,3],p = p.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(parameters,char_to_ix,seed):\n",
    "    '''\n",
    "    根据rnn输出的概率分布序列对字符序列进行采样\n",
    "    参数：\n",
    "        parameters -- 包含了Waa，Wax，Wya，by,b的字典\n",
    "        char_to_ix -- 字符映射到索引的字典\n",
    "        seed -- 随机种子\n",
    "        \n",
    "    返回：\n",
    "        indices -- 包含采样字符索引的长度为n的列表。\n",
    "    '''\n",
    "    # 从parameters 中获取参数\n",
    "    Waa,Wax,Wya,by,b = parameters['Waa'],parameters['Wax'],parameters['Wya'],parameters['by'],parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    # 创建独热向量\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # 使用0初始化a_prev\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    indices = []\n",
    "    # 检测换行符\n",
    "    idx = -1\n",
    "    # 循环遍历时间步骤t。在每个时间步中，从概率分布中抽取一个字符，\n",
    "    # 并将其索引附加到“indices”上，如果我们达到50个字符，\n",
    "    #（我们应该不太可能有一个训练好的模型），我们将停止循环，这有助于调试并防止进入无限循环\n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while(idx != newline_character and counter < 50):\n",
    "        # 前向传播\n",
    "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev) + b)\n",
    "        z = np.dot(Wya,a)+by\n",
    "        y = cllm_utils.softmax(z)\n",
    "        # 设定随机种子\n",
    "        np.random.seed(seed+counter)\n",
    "        # 从概率分布中抽取词汇表字符的索引\n",
    "        idx = np.random.choice(list(range(vocab_size)),p = y.ravel())\n",
    "        # \n",
    "        indices.append(idx)\n",
    "        # 步骤4:将输入字符重写为与采样索引对应的字符。\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        #更新a_prev 为a\n",
    "        a_prev = a\n",
    "        # 累加器\n",
    "        seed += 1\n",
    "        counter += 1\n",
    "        \n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 17, 24, 1, 13, 0, 0]\n",
      "list of sampled characters: ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', 'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', 'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'q', 'x', 'a', 'm', '\\n', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "_, n_a = 20, 100\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(X,Y,a_prev,parameters,learning_rate=0.01):\n",
    "    '''\n",
    "    执行训练模型的单步优化。\n",
    "    \n",
    "    参数：\n",
    "        X -- 整数列表，其中每个整数映射到词汇表中的字符。\n",
    "        Y -- 整数列表，与X完全相同，但向左移动了一个索引。\n",
    "        a_prev -- 上一个隐藏状态\n",
    "        parameters -- 字典，包含了以下参数：\n",
    "                        Wax -- 权重矩阵乘以输入，维度为(n_a, n_x)\n",
    "                        Waa -- 权重矩阵乘以隐藏状态，维度为(n_a, n_a)\n",
    "                        Wya -- 隐藏状态与输出相关的权重矩阵，维度为(n_y, n_a)\n",
    "                        b -- 偏置，维度为(n_a, 1)\n",
    "                        by -- 隐藏状态与输出相关的权重偏置，维度为(n_y, 1)\n",
    "        learning_rate -- 模型学习的速率\n",
    "    \n",
    "    返回：\n",
    "        loss -- 损失函数的值（交叉熵损失）\n",
    "        gradients -- 字典，包含了以下参数：\n",
    "                        dWax -- 输入到隐藏的权值的梯度，维度为(n_a, n_x)\n",
    "                        dWaa -- 隐藏到隐藏的权值的梯度，维度为(n_a, n_a)\n",
    "                        dWya -- 隐藏到输出的权值的梯度，维度为(n_y, n_a)\n",
    "                        db -- 偏置的梯度，维度为(n_a, 1)\n",
    "                        dby -- 输出偏置向量的梯度，维度为(n_y, 1)\n",
    "        a[len(X)-1] -- 最后的隐藏状态，维度为(n_a, 1)\n",
    "    '''\n",
    "    # 前向传播\n",
    "    loss,cache = cllm_utils.rnn_forward(X,Y,a_prev,parameters)\n",
    "    # 反向传播\n",
    "    gradients,a = cllm_utils.rnn_backward(X,Y,parameters,cache)\n",
    "    # 梯度修剪\n",
    "    gradients = clip(gradients,5)\n",
    "    # 更新参数\n",
    "    parameters = cllm_utils.update_parameters(parameters,gradients,learning_rate)\n",
    "    \n",
    "    return loss,gradients,a[len(X) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165382\n",
      "gradients[\"dWaa\"][1][2] = 0.1947093153471637\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032002977\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data,ix_to_char,char_to_ix,num_iterations=3500,\n",
    "         n_a=50,dino_names=70,vocab_size=27):\n",
    "    '''\n",
    "    训练模型并生成恐龙的名字\n",
    "    参数：\n",
    "        data -- 语料库\n",
    "        ix_to_char -- 索引映射字符字典\n",
    "        char_to_ix -- 字符映射索引字典\n",
    "        num_iterations -- 迭代次数\n",
    "        n_a -- RNN单元数量\n",
    "        dino_names -- 每次迭代中采样的数量\n",
    "        vocab_size -- 在文本中的唯一字符的数量\n",
    "    \n",
    "    返回：\n",
    "        parameters -- 学习后了的参数\n",
    "    '''\n",
    "    n_x,n_y = vocab_size,vocab_size\n",
    "    #初始化参数\n",
    "    parameters = cllm_utils.initialize_parameters(n_a,n_x,n_y)\n",
    "    # 初始化损失\n",
    "    loss = cllm_utils.get_initial_loss(vocab_size,dino_names)\n",
    "    # 构建恐龙名称列表\n",
    "    with open('dinos.txt') as f:\n",
    "        examples = f.readline()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    # 打乱全部恐龙名称\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(examples)\n",
    "    # 初始化LSTM隐藏状态\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    # 循环\n",
    "    for j in range(num_iterations):\n",
    "        #定义一个训练样本\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n",
    "        Y = X[1:] + [char_to_ix['\\n']]\n",
    "        # 学习\n",
    "        curr_loss,gradients,a_prev = optimize(X,Y,a_prev,parameters)\n",
    "        # 使用延迟来保持损失平滑,这是为了加速训练。\n",
    "        loss = cllm_utils.smooth(loss,curr_loss)\n",
    "        #迭代\n",
    "        if j % 2000 == 0:\n",
    "            print('第'+str(j+1)+'次迭代，损失值为：'+str(loss))\n",
    "            \n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                sampled_indices = sample(parameters,char_to_ix,seed)\n",
    "                cllm_utils.print_sample(sampled_indices,ix_to_char)\n",
    "                \n",
    "                seed+=1\n",
    "                \n",
    "            print('\\n')\n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1次迭代，损失值为：230.48446307158778\n",
      "Nkzxwtdmfqoeyhsqwasjjjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjjjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjjjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjjjvu\n",
      "B\n",
      "Wtdmfqoeyhsqwasjjjvu\n",
      "\n",
      "Tdmfqoeyhsqwasjjjvu\n",
      "Dtvgbamwkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdp\n",
      "Dmfqoeyhsqwasjjjvu\n",
      "Tvgbamwkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdpp\n",
      "Mfqoeyhsqwasjjjvu\n",
      "Vgbamwkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppw\n",
      "Fqoeyhsqwasjjjvu\n",
      "Gbamwkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwe\n",
      "Qoeyhsqwasjjjvu\n",
      "Bamwkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwel\n",
      "Oeyhsqwasjjjvu\n",
      "Amwkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelv\n",
      "Eyhsqwasjjjvu\n",
      "Mwkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvb\n",
      "Yhsqwasjjjvu\n",
      "Wkwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbr\n",
      "Hsqwasjjjvu\n",
      "Kwgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrm\n",
      "Sqwasjjjvu\n",
      "Wgflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmn\n",
      "Qwasjjjvu\n",
      "Gflynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmno\n",
      "Wasjjjvu\n",
      "Flynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoq\n",
      "Asjjjvu\n",
      "Lynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqk\n",
      "Sjjjvu\n",
      "Ynfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkq\n",
      "Jjjvu\n",
      "Nfczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqy\n",
      "Jjvu\n",
      "Fczchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqym\n",
      "Jvu\n",
      "Czchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymx\n",
      "Vu\n",
      "Zchrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxs\n",
      "U\n",
      "Chrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsg\n",
      "\n",
      "Hrvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgi\n",
      "Mvkzih\n",
      "Rvbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiw\n",
      "Vkzih\n",
      "Vbbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwt\n",
      "Kzih\n",
      "Bbxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtc\n",
      "Zih\n",
      "Bxvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtcq\n",
      "Ih\n",
      "Xvnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtcqu\n",
      "H\n",
      "Vnengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtcquu\n",
      "\n",
      "Nengdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtcquuy\n",
      "Jdfybehangaeqdwsesnpc\n",
      "Engdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtcquuyr\n",
      "Dfybehangaeqdwsesnpc\n",
      "Ngdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtcquuyrv\n",
      "Fybehangaeqdwsesnpc\n",
      "Gdqoxmifpqmepfvrmkbdppwelvbrmnoqkqymxsgiwtcquuyrvx\n",
      "\n",
      "\n",
      "第2001次迭代，损失值为：34.09093782382851\n",
      "A\n",
      "\n",
      "\n",
      "A\n",
      "Wuor\n",
      "\n",
      "Sso\n",
      "\n",
      "Sl\n",
      "\n",
      "N\n",
      "\n",
      "\n",
      "Ns\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "Ec\n",
      "\n",
      "B\n",
      "\n",
      "\n",
      "\n",
      "V\n",
      "S\n",
      "\n",
      "\n",
      "Kls\n",
      "S\n",
      "Es\n",
      "\n",
      "S\n",
      "\n",
      "\n",
      "\n",
      "Ka\n",
      "U\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ro\n",
      "Y\n",
      "O\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Gs\n",
      "Qaua\n",
      "R\n",
      "\n",
      "\n",
      "Ya\n",
      "\n",
      "\n",
      "Uoe\n",
      "\n",
      "Qe\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "执行了：0分1秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "parameters = model(data,ix_to_char,char_to_ix,num_iterations=3500)\n",
    "end_time = time.clock()\n",
    "minium = end_time - start_time\n",
    "print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n",
      "执行了：0分3秒\n"
     ]
    }
   ],
   "source": [
    "#开始时间\n",
    "start_time = time.clock()\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io\n",
    "\n",
    "#结束时间\n",
    "end_time = time.clock()\n",
    "\n",
    "#计算时差\n",
    "minium = end_time - start_time\n",
    "\n",
    "print(\"执行了：\" + str(int(minium / 60)) + \"分\" + str(int(minium%60)) + \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31412/31412 [==============================] - 162s - loss: 2.5502   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21d600f54a8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end = on_epoch_end)\n",
    "model.fit(x,y,batch_size=128,epochs=1,callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: cloud\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "clouds.\n",
      "when thou been, and withrouts terver fors,\n",
      "and worme ouht, in thou faald heard's ness,\n",
      "i to prut fer seaw,, this, thit  'that on thy days gid dant spent,\n",
      "for that i hads menfand doss, doth no', whore wisk me:\n",
      "like, you but pocfelte farte day me as fance,\n",
      "to ar with tonve thy dayamerss great me.\n",
      "\n",
      "who sid being for thy healls ladd forsed\n",
      "i dis melive aming nidefe,\n",
      "of to me, faked the counloer con"
     ]
    }
   ],
   "source": [
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用LSTM网络即兴独奏爵士乐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
